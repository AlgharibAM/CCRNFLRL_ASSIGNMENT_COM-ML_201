{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYpYIJK8/alQawepT9A9qE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlgharibAM/CCRNFLRL_ASSIGNMENT_COM-ML_201/blob/main/exercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "OYV_xSLj4Fsk",
        "outputId": "3acf3252-57f3-47ab-dd24-08ecaca0af21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2712cb29c949>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   {\n\u001b[1;32m      4\u001b[0m    \u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"code\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m    \u001b[0;34m\"execution_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m    \"metadata\": {\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"collapsed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'null' is not defined"
          ]
        }
      ],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"collapsed\": true\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"%matplotlib inline\\n\",\n",
        "    \"\\n\",\n",
        "    \"import gym\\n\",\n",
        "    \"import matplotlib\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import sys\\n\",\n",
        "    \"\\n\",\n",
        "    \"from collections import defaultdict\\n\",\n",
        "    \"\\n\",\n",
        "    \"if \\\"../\\\" not in sys.path:\\n\",\n",
        "    \"  sys.path.append(\\\"../\\\") \\n\",\n",
        "    \"from lib.envs.blackjack import BlackjackEnv\\n\",\n",
        "    \"from lib import plotting\\n\",\n",
        "    \"\\n\",\n",
        "    \"matplotlib.style.use('ggplot')\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"collapsed\": true\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"env = BlackjackEnv()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"collapsed\": true\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Monte Carlo prediction algorithm. Calculates the value function\\n\",\n",
        "    \"    for a given policy using sampling.\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    Args:\\n\",\n",
        "    \"        policy: A function that maps an observation to action probabilities.\\n\",\n",
        "    \"        env: OpenAI gym environment.\\n\",\n",
        "    \"        num_episodes: Number of episodes to sample.\\n\",\n",
        "    \"        discount_factor: Gamma discount factor.\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    Returns:\\n\",\n",
        "    \"        A dictionary that maps from state -> value.\\n\",\n",
        "    \"        The state is a tuple and the value is a float.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"    # Keeps track of sum and count of returns for each state\\n\",\n",
        "    \"    # to calculate an average. We could use an array to save all\\n\",\n",
        "    \"    # returns (like in the book) but that's memory inefficient.\\n\",\n",
        "    \"    returns_sum = defaultdict(float)\\n\",\n",
        "    \"    returns_count = defaultdict(float)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # The final value function\\n\",\n",
        "    \"    V = defaultdict(float)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Implement this!\\n\",\n",
        "    \"\\n\",\n",
        "    \"    for i_episode in range(1, num_episodes+1):\\n\",\n",
        "    \"        # Print out which episode we're on, useful for debugging.\\n\",\n",
        "    \"        if i_episode % 1000 == 0:\\n\",\n",
        "    \"            print(\\\"\\\\rEpisode {}/{}.\\\".format(i_episode, num_episodes), end=\\\"\\\")\\n\",\n",
        "    \"            sys.stdout.flush()\\n\",\n",
        "    \"        # Generate an episode.\\n\",\n",
        "    \"        episode = []\\n\",\n",
        "    \"        state = env.reset()\\n\",\n",
        "    \"        for t in range(100):\\n\",\n",
        "    \"            action = policy(state)\\n\",\n",
        "    \"            next_state, reward, done, _ = env.step(action)\\n\",\n",
        "    \"            episode.append((state, action, reward))\\n\",\n",
        "    \"            if done:\\n\",\n",
        "    \"                break\\n\",\n",
        "    \"            state = next_state\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Find all states the we've visited in this episode\\n\",\n",
        "    \"        # We convert each state to a tuple so that we can use it as a dict key\\n\",\n",
        "    \"        states_in_episode = set([tuple(x[0]) for x in episode])\\n\",\n",
        "    \"        for state in states_in_episode:\\n\",\n",
        "    \"            # Find the first occurance of the state in the episode\\n\",\n",
        "    \"            first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == state)\\n\",\n",
        "    \"            # Sum up all rewards since the first occurance\\n\",\n",
        "    \"            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\\n\",\n",
        "    \"            # Calculate average return for this state over all sampled episodes\\n\",\n",
        "    \"            returns_sum[state] += G\\n\",\n",
        "    \"            returns_count[state] += 1.0\\n\",\n",
        "    \"            V[state] = returns_sum[state] / returns_count[state]\\n\",\n",
        "    \"\\n\",\n",
        "    \"\\n\",\n",
        "    \"        \\n\",\n",
        "    \"\\n\",\n",
        "    \"    return V    \"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"collapsed\": true\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"def sample_policy(observation):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    A policy that sticks if the player score is > 20 and hits otherwise.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    score, dealer_score, usable_ace = observation\\n\",\n",
        "    \"    return 0 if score >= 20 else 1\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"collapsed\": true,\n",
        "    \"scrolled\": false\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"V_10k = mc_prediction(sample_policy, env, num_episodes=10000)\\n\",\n",
        "    \"plotting.plot_value_function(V_10k, title=\\\"10,000 Steps\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"V_500k = mc_prediction(sample_policy, env, num_episodes=500000)\\n\",\n",
        "    \"plotting.plot_value_function(V_500k, title=\\\"500,000 Steps\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"collapsed\": true\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": []\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.5.2\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 1\n",
        "}"
      ]
    }
  ]
}